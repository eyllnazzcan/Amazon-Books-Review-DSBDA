{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f727fa0-5300-4258-b174-dafcbfbf6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "463c00dc-3c0e-4967-b162-7da0d19b1e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonReviewsIngestion\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "262d0dda-9711-48cc-a284-110caf6477e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Schema for the dataset, specifying the data types for cols\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_title\", StringType(), True),\n",
    "    StructField(\"star_rating\", StringType(), True),\n",
    "    StructField(\"helpful_votes\", StringType(), True),\n",
    "    StructField(\"total_votes\", StringType(), True),\n",
    "    StructField(\"review_headline\", StringType(), True),\n",
    "    StructField(\"review_body\", StringType(), True),\n",
    "    StructField(\"review_date\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e98bf0ad-6e6e-4485-986a-03cd4b5dc9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample:\n",
      "+----------+--------------------+-----------+-------------+-----------+--------------------+--------------------+-----------+\n",
      "|product_id|       product_title|star_rating|helpful_votes|total_votes|     review_headline|         review_body|review_date|\n",
      "+----------+--------------------+-----------+-------------+-----------+--------------------+--------------------+-----------+\n",
      "|0312977379|    Beware the Night|        4.0|         61.0|       79.0|A book that actua...|Unlike many books...| 2005-10-13|\n",
      "|1420832158|JEET KUNE DO: THE...|        5.0|          1.0|        4.0|Something  For Ev...|This book is the ...| 2005-10-13|\n",
      "|0312977379|    Beware the Night|        5.0|         12.0|       18.0|    Beware the Night|When I started th...| 2005-10-13|\n",
      "|0312336853|Shooter: The Auto...|        5.0|          1.0|        4.0|Hard to put this ...|This book has som...| 2005-10-13|\n",
      "|0756607574|             Panties|        4.0|          5.0|       12.0|         A Nice Read|This book is a sm...| 2005-10-13|\n",
      "+----------+--------------------+-----------+-------------+-----------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: string (nullable = true)\n",
      " |-- helpful_votes: string (nullable = true)\n",
      " |-- total_votes: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Local path to the dataset\n",
    "local_file_path = \"clean_books_10k.csv\"\n",
    "\n",
    "reviews_df = spark.read.csv(\n",
    "    local_file_path,\n",
    "    sep=\",\",\n",
    "    header=True,       \n",
    "    schema=schema,     # Fixed schema\n",
    "    multiLine=True,\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "print(\"Data Sample:\")\n",
    "reviews_df.show(5)\n",
    "\n",
    "print(\"Schema:\")\n",
    "reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07df3a23-5542-43ee-af0e-1cfbb379e456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to HDFS at hdfs://localhost:54310/user/ubuntu/books_dataset/\n"
     ]
    }
   ],
   "source": [
    "# HDFS output directory\n",
    "hdfs_path = \"hdfs://localhost:54310/user/ubuntu/books_dataset/\"\n",
    "\n",
    "# Uses coalesce to ensure the data is written as a single file\n",
    "reviews_df.coalesce(1).write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(hdfs_path)\n",
    "\n",
    "print(f\"Data written to HDFS at {hdfs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e78a9cf-f07b-4031-88ba-048509537160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory-based streaming started. Copy CSV chunks into 'streaming_folder/' to see updates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 22:31:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2ac2002b-a88d-4345-a5ae-7ccdd0e7285c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/09/25 22:31:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  117|\n",
      "|        5.0|  560|\n",
      "|        4.0|  184|\n",
      "|        2.0|   50|\n",
      "|        3.0|   89|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  219|\n",
      "|        5.0| 1116|\n",
      "|        4.0|  380|\n",
      "|        2.0|  110|\n",
      "|        3.0|  175|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  318|\n",
      "|        5.0| 1702|\n",
      "|        4.0|  559|\n",
      "|        2.0|  175|\n",
      "|        3.0|  246|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  421|\n",
      "|        5.0| 2286|\n",
      "|        4.0|  730|\n",
      "|        2.0|  227|\n",
      "|        3.0|  336|\n",
      "+-----------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  510|\n",
      "|        5.0| 2839|\n",
      "|        4.0|  916|\n",
      "|        2.0|  305|\n",
      "|        3.0|  430|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  569|\n",
      "|        5.0| 3446|\n",
      "|        4.0| 1123|\n",
      "|        2.0|  357|\n",
      "|        3.0|  505|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  655|\n",
      "|        5.0| 3993|\n",
      "|        4.0| 1324|\n",
      "|        2.0|  429|\n",
      "|        3.0|  599|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  728|\n",
      "|        5.0| 4578|\n",
      "|        4.0| 1535|\n",
      "|        2.0|  478|\n",
      "|        3.0|  681|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  801|\n",
      "|        5.0| 5157|\n",
      "|        4.0| 1741|\n",
      "|        2.0|  536|\n",
      "|        3.0|  765|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  884|\n",
      "|        5.0| 5818|\n",
      "|        4.0| 1862|\n",
      "|        2.0|  590|\n",
      "|        3.0|  846|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_input_path = \"hdfs://localhost:54310/user/ubuntu/streaming_input/\"\n",
    "\n",
    "# Create streaming DF to read CSV file\n",
    "streaming_df = spark.readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"header\", False) \\\n",
    "    .option(\"multiLine\", True) \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(streaming_input_path)\n",
    "\n",
    "streaming_df = streaming_df.filter(streaming_df[\"product_id\"] != \"product_id\")\n",
    "\n",
    "# Example query: count reviews by rating as files arrive\n",
    "query = streaming_df.groupBy(\"star_rating\").count()\n",
    "\n",
    "query_writer = query.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Directory-based streaming started. Copy CSV chunks into 'streaming_folder/' to see updates.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
