{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f727fa0-5300-4258-b174-dafcbfbf6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463c00dc-3c0e-4967-b162-7da0d19b1e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 01:43:34 WARN Utils: Your hostname, dsbda-vm resolves to a loopback address: 127.0.1.1; using 192.168.64.3 instead (on interface enp0s1)\n",
      "25/09/25 01:43:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/25 01:43:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonReviewsIngestion\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "262d0dda-9711-48cc-a284-110caf6477e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Schema for the dataset, specifying the data types for cols\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_title\", StringType(), True),\n",
    "    StructField(\"star_rating\", StringType(), True),\n",
    "    StructField(\"helpful_votes\", StringType(), True),\n",
    "    StructField(\"total_votes\", StringType(), True),\n",
    "    StructField(\"review_headline\", StringType(), True),\n",
    "    StructField(\"review_body\", StringType(), True),\n",
    "    StructField(\"review_date\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98bf0ad-6e6e-4485-986a-03cd4b5dc9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample:\n",
      "+----------+--------------------+-----------+-------------+-----------+--------------------+--------------------+-----------+\n",
      "|product_id|       product_title|star_rating|helpful_votes|total_votes|     review_headline|         review_body|review_date|\n",
      "+----------+--------------------+-----------+-------------+-----------+--------------------+--------------------+-----------+\n",
      "|0312977379|    Beware the Night|        4.0|         61.0|       79.0|A book that actua...|Unlike many books...| 2005-10-13|\n",
      "|1420832158|JEET KUNE DO: THE...|        5.0|          1.0|        4.0|Something  For Ev...|This book is the ...| 2005-10-13|\n",
      "|0312977379|    Beware the Night|        5.0|         12.0|       18.0|    Beware the Night|When I started th...| 2005-10-13|\n",
      "|0312336853|Shooter: The Auto...|        5.0|          1.0|        4.0|Hard to put this ...|This book has som...| 2005-10-13|\n",
      "|0756607574|             Panties|        4.0|          5.0|       12.0|         A Nice Read|This book is a sm...| 2005-10-13|\n",
      "+----------+--------------------+-----------+-------------+-----------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: string (nullable = true)\n",
      " |-- helpful_votes: string (nullable = true)\n",
      " |-- total_votes: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Local path to the dataset\n",
    "local_file_path = \"clean_books_10k.csv\"\n",
    "\n",
    "reviews_df = spark.read.csv(\n",
    "    local_file_path,\n",
    "    sep=\",\",\n",
    "    header=True,       \n",
    "    schema=schema,     # Fixed schema\n",
    "    multiLine=True,\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "print(\"Data Sample:\")\n",
    "reviews_df.show(5)\n",
    "\n",
    "print(\"Schema:\")\n",
    "reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07df3a23-5542-43ee-af0e-1cfbb379e456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to HDFS at hdfs://localhost:54310/user/ubuntu/books_dataset/\n"
     ]
    }
   ],
   "source": [
    "# HDFS output directory\n",
    "hdfs_path = \"hdfs://localhost:54310/user/ubuntu/books_dataset/\"\n",
    "\n",
    "reviews_df.coalesce(1).write \\ # Uses coalesce to ensure the data is written as a single file\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(hdfs_path)\n",
    "\n",
    "print(f\"Data written to HDFS at {hdfs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2fc89c-d1e2-4f4c-8114-1f248647a0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded from HDFS:\n",
      "+----------+--------------------+-----------+-------------+-----------+--------------------+--------------------+-----------+\n",
      "|product_id|       product_title|star_rating|helpful_votes|total_votes|     review_headline|         review_body|review_date|\n",
      "+----------+--------------------+-----------+-------------+-----------+--------------------+--------------------+-----------+\n",
      "|0312977379|    Beware the Night|        4.0|         61.0|       79.0|A book that actua...|Unlike many books...| 2005-10-13|\n",
      "|1420832158|JEET KUNE DO: THE...|        5.0|          1.0|        4.0|Something  For Ev...|This book is the ...| 2005-10-13|\n",
      "|0312977379|    Beware the Night|        5.0|         12.0|       18.0|    Beware the Night|When I started th...| 2005-10-13|\n",
      "|0312336853|Shooter: The Auto...|        5.0|          1.0|        4.0|Hard to put this ...|This book has som...| 2005-10-13|\n",
      "|0756607574|             Panties|        4.0|          5.0|       12.0|         A Nice Read|This book is a sm...| 2005-10-13|\n",
      "+----------+--------------------+-----------+-------------+-----------+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the write action by reading back\n",
    "reloaded_df = spark.read.csv(\n",
    "    hdfs_path,\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    schema=schema,\n",
    "    multiLine=True,\n",
    "    quote='\"',\n",
    "    escape=\"\\\\\"\n",
    ")\n",
    "\n",
    "print(\"Reloaded from HDFS:\")\n",
    "reloaded_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e78a9cf-f07b-4031-88ba-048509537160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory-based streaming started. Copy CSV chunks into 'streaming_folder/' to see updates.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 01:49:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-12d058e3-2152-4b52-9275-db500c828b20. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/09/25 01:49:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  117|\n",
      "|        5.0|  560|\n",
      "|        4.0|  184|\n",
      "|        2.0|   50|\n",
      "|        3.0|   89|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 01:49:49 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 5.0\n",
      " Schema: star_rating\n",
      "Expected: star_rating but found: 5.0\n",
      "CSV file: hdfs://localhost:54310/user/ubuntu/streaming_input/part_1.csv\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  219|\n",
      "|        5.0| 1115|\n",
      "|        4.0|  380|\n",
      "|        2.0|  110|\n",
      "|        3.0|  175|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 01:50:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 5.0\n",
      " Schema: star_rating\n",
      "Expected: star_rating but found: 5.0\n",
      "CSV file: hdfs://localhost:54310/user/ubuntu/streaming_input/part_2.csv\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  318|\n",
      "|        5.0| 1700|\n",
      "|        4.0|  559|\n",
      "|        2.0|  175|\n",
      "|        3.0|  246|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 01:50:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 4.0\n",
      " Schema: star_rating\n",
      "Expected: star_rating but found: 4.0\n",
      "CSV file: hdfs://localhost:54310/user/ubuntu/streaming_input/part_3.csv\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  421|\n",
      "|        5.0| 2284|\n",
      "|        4.0|  729|\n",
      "|        2.0|  227|\n",
      "|        3.0|  336|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 01:50:21 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 4.0\n",
      " Schema: star_rating\n",
      "Expected: star_rating but found: 4.0\n",
      "CSV file: hdfs://localhost:54310/user/ubuntu/streaming_input/part_4.csv\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  510|\n",
      "|        5.0| 2837|\n",
      "|        4.0|  914|\n",
      "|        2.0|  305|\n",
      "|        3.0|  430|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 01:50:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 5.0\n",
      " Schema: star_rating\n",
      "Expected: star_rating but found: 5.0\n",
      "CSV file: hdfs://localhost:54310/user/ubuntu/streaming_input/part_5.csv\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  569|\n",
      "|        5.0| 3443|\n",
      "|        4.0| 1121|\n",
      "|        2.0|  357|\n",
      "|        3.0|  505|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 01:50:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 4.0\n",
      " Schema: star_rating\n",
      "Expected: star_rating but found: 4.0\n",
      "CSV file: hdfs://localhost:54310/user/ubuntu/streaming_input/part_6.csv\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  655|\n",
      "|        5.0| 3990|\n",
      "|        4.0| 1321|\n",
      "|        2.0|  429|\n",
      "|        3.0|  599|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 01:50:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 5.0\n",
      " Schema: star_rating\n",
      "Expected: star_rating but found: 5.0\n",
      "CSV file: hdfs://localhost:54310/user/ubuntu/streaming_input/part_7.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  728|\n",
      "|        5.0| 4574|\n",
      "|        4.0| 1532|\n",
      "|        2.0|  478|\n",
      "|        3.0|  681|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 01:51:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 2.0\n",
      " Schema: star_rating\n",
      "Expected: star_rating but found: 2.0\n",
      "CSV file: hdfs://localhost:54310/user/ubuntu/streaming_input/part_8.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  801|\n",
      "|        5.0| 5153|\n",
      "|        4.0| 1738|\n",
      "|        2.0|  535|\n",
      "|        3.0|  765|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/25 01:51:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 5.0\n",
      " Schema: star_rating\n",
      "Expected: star_rating but found: 5.0\n",
      "CSV file: hdfs://localhost:54310/user/ubuntu/streaming_input/part_9.csv\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+-----------+-----+\n",
      "|star_rating|count|\n",
      "+-----------+-----+\n",
      "|        1.0|  884|\n",
      "|        5.0| 5813|\n",
      "|        4.0| 1859|\n",
      "|        2.0|  589|\n",
      "|        3.0|  846|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_input_path = \"hdfs://localhost:54310/user/ubuntu/streaming_input/\"\n",
    "\n",
    "# Create streaming DF to read CSV file\n",
    "streaming_df = spark.readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"multiLine\", True) \\ \n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(streaming_input_path)\n",
    "\n",
    "# Example query: count reviews by rating as files arrive\n",
    "query = streaming_df.groupBy(\"star_rating\").count()\n",
    "\n",
    "query_writer = query.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Directory-based streaming started. Copy CSV chunks into 'streaming_folder/' to see updates.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
